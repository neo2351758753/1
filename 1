import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, NamedTuple, Union
import copy
import math
import time
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import Dataset, DataLoader
import logging
import os
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm  # 引入 tqdm

# 配置日志
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(message)s')


# ----------------------------
# 一、定义棋盘类 ChessBoard
# ----------------------------
class ChessBoard:
    def __init__(self, board_len: int = 15, n_feature_planes: int = 4, search_radius: int = 2):
        """
        初始化棋盘。

        Args:
            board_len (int): 棋盘边长。
            n_feature_planes (int): 特征平面的数量。
            search_radius (int): 搜索半径，用于限制可选动作的范围。
        """
        self.board_len = board_len
        self.current_player = 1  # 1 for black, -1 for white
        self.state = np.zeros((board_len, board_len), dtype=np.int8)
        self.n_feature_planes = n_feature_planes
        self.search_radius = search_radius
        self.available_actions = set(range(board_len * board_len))
        self.candidate_positions = set()
        self.move_history = []

    def clear_board(self):
        """
        清空棋盘，重置游戏状态。
        """
        self.current_player = 1
        self.state.fill(0)
        self.available_actions = set(range(self.board_len * self.board_len))
        self.candidate_positions = set()
        self.move_history = []

    def clone(self) -> 'ChessBoard':
        """
        克隆当前棋盘状态。

        Returns:
            ChessBoard: 克隆后的棋盘实例。
        """
        new_board = ChessBoard(self.board_len, self.n_feature_planes, self.search_radius)
        new_board.state = self.state.copy()
        new_board.current_player = self.current_player
        new_board.available_actions = self.available_actions.copy()
        new_board.candidate_positions = self.candidate_positions.copy()
        new_board.move_history = self.move_history.copy()
        return new_board

    def get_available_actions(self) -> List[int]:
        """
        获取当前棋盘上所有可用的动作。

        Returns:
            List[int]: 可用动作的列表。
        """
        if not self.move_history:
            return list(self.available_actions)
        return list(self.candidate_positions.intersection(self.available_actions))

    def do_action(self, action: int, player: int = None) -> None:
        """
        执行动作，将棋子放置在指定位置。

        Args:
            action (int): 要执行的动作。
            player (int, optional): 指定执行动作的玩家。如果未指定，则使用当前玩家并切换玩家。

        Raises:
            ValueError: 如果动作不可用。
        """
        if action not in self.get_available_actions():
            raise ValueError(f"Action {action} is not available")
        if player is None:
            player = self.current_player
            self.current_player *= -1  # 切换玩家
        x, y = divmod(action, self.board_len)
        self.state[x, y] = player
        self.available_actions.remove(action)
        self.move_history.append(action)

        # 更新候选位置
        for dx in range(-self.search_radius, self.search_radius + 1):
            for dy in range(-self.search_radius, self.search_radius + 1):
                nx, ny = x + dx, y + dy
                if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
                    pos = nx * self.board_len + ny
                    if self.state[nx, ny] == 0:
                        self.candidate_positions.add(pos)

    def undo_action(self):
        """
        撤销最后一个动作。
        """
        if self.move_history:
            action = self.move_history.pop()
            x, y = divmod(action, self.board_len)
            self.state[x, y] = 0
            self.current_player *= -1
            self.available_actions.add(action)
            # 重新计算候选位置
            self.candidate_positions.clear()
            for move in self.move_history:
                mx, my = divmod(move, self.board_len)
                for dx in range(-self.search_radius, self.search_radius + 1):
                    for dy in range(-self.search_radius, self.search_radius + 1):
                        nx, ny = mx + dx, my + dy
                        if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
                            pos = nx * self.board_len + ny
                            if self.state[nx, ny] == 0:
                                self.candidate_positions.add(pos)

    def is_game_over(self) -> Tuple[bool, Union[int, None]]:
        """
        检查游戏是否结束。

        Returns:
            Tuple[bool, Union[int, None]]: (游戏是否结束, 胜利者)
        """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        for x in range(self.board_len):
            for y in range(self.board_len):
                player = self.state[x, y]
                if player == 0:
                    continue
                for dx, dy in directions:
                    count = 1
                    # 正方向
                    for step in range(1, 5):
                        nx, ny = x + dx * step, y + dy * step
                        if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
                            if self.state[nx, ny] == player:
                                count += 1
                            else:
                                break
                        else:
                            break
                    if count >= 5:
                        return True, player
        if not self.get_available_actions():
            return True, None  # 平局
        return False, None

    def get_feature_planes(self) -> torch.Tensor:
        """
        获取当前棋盘的特征平面。

        Returns:
            torch.Tensor: 特征平面张量。
        """
        planes = np.zeros((self.n_feature_planes, self.board_len, self.board_len), dtype=np.float32)
        actions = np.argwhere(self.state != 0)
        if actions.size > 0:
            x = actions[:, 0]
            y = actions[:, 1]
            player_values = (self.state[x, y] == self.current_player).astype(np.float32)
            planes[0, x, y] = player_values

            opponent_values = (self.state[x, y] == -self.current_player).astype(np.float32)
            planes[1, x, y] = opponent_values

        available = np.array(list(self.get_available_actions()))
        if available.size > 0:
            x_available = available // self.board_len
            y_available = available % self.board_len
            planes[2, x_available, y_available] = 1.0

        # 添加启发式评分作为额外的特征平面
        if self.n_feature_planes > 3:
            heuristic_scores = self.compute_heuristic_scores()
            planes[3] = heuristic_scores

        return torch.tensor(planes, dtype=torch.float32)

    def compute_heuristic_scores(self) -> np.ndarray:
        """
        计算棋盘上每个可用位置的启发式评分。

        Returns:
            np.ndarray: 启发式评分网格。
        """
        available_actions = self.get_available_actions()
        scores = np.zeros((self.board_len, self.board_len), dtype=np.float32)
        if not available_actions:
            return scores

        actions = np.array(available_actions)
        xs, ys = divmod(actions, self.board_len)
        # 使用列表推导式进行批量评估
        scores_list = [self.evaluate_action(self.clone(), action, self.current_player) for action in available_actions]
        scores[xs, ys] = scores_list
        return scores

    def evaluate_action(self, board: 'ChessBoard', action: int, player: int) -> float:
        """
        评估一个动作的得分。

        Args:
            board (ChessBoard): 当前棋盘。
            action (int): 要评估的动作。
            player (int): 玩家标识。

        Returns:
            float: 动作的得分。
        """
        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
        score_table = {
            'existing_connections': {1: 1, 2: 5, 3: 50, 4: 10000},
            'live_connections': {1: 10, 2: 100, 3: 500, 4: 10000},
            'dead_connections': {1: 1, 2: 10, 3: 100, 4: 5000},
            'jump_three': 200,
            'jump_four': 5000,
            'self_block': -50,
            'opponent_block': -100,
            'mixed_ends': 100,
            'block_opponent_levels': {
                2: 20,
                3: 100,
                4: 1000,
                5: 5000
            }
        }

        total_score = 0.0
        x, y = divmod(action, board.board_len)

        for dx, dy in directions:
            conn_info = board.check_connection(x, y, dx, dy, player)
            if conn_info:
                existing_conn, live_conn, dead_conn, ends = conn_info
                # 基础得分
                total_score += (
                    score_table['existing_connections'].get(existing_conn, 0) +
                    score_table['live_connections'].get(live_conn, 0) +
                    score_table['dead_connections'].get(dead_conn, 0)
                )
                # 特殊末端状态得分
                if ends == (1, 1):
                    total_score += score_table['self_block']
                elif ends == (-1, -1):
                    total_score += score_table['opponent_block']
                elif ends in [(1, -1), (-1, 1)]:
                    total_score += score_table['mixed_ends']

            # 检测跳三、跳四
            jump_three, jump_four = board.check_advanced_patterns(x, y, dx, dy, player)
            total_score += jump_three * score_table['jump_three']
            total_score += jump_four * score_table['jump_four']

        # 新增阻挡对手逻辑
        opponent = -player
        for dx, dy in directions:
            opponent_line_len = board.check_opponent_block(x, y, dx, dy, player)
            for length_threshold, block_score in sorted(score_table['block_opponent_levels'].items(), reverse=True):
                if opponent_line_len >= length_threshold:
                    total_score += block_score
                    break

        return total_score

    def check_advanced_patterns(self, x: int, y: int, dx: int, dy: int, player: int) -> Tuple[int, int]:
        """
        检测高级模式（跳三、跳四）。

        Args:
            x (int): 棋子x坐标。
            y (int): 棋子y坐标。
            dx (int): x方向增量。
            dy (int): y方向增量。
            player (int): 玩家标识。

        Returns:
            Tuple[int, int]: (跳三数量, 跳四数量)
        """
        jump_three = 0
        jump_four = 0
        pattern = []

        for i in range(-4, 5):
            nx, ny = x + i * dx, y + i * dy
            if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
                pattern.append(self.state[nx, ny])
            else:
                pattern.append(None)

        # 检测跳三
        for i in range(len(pattern) - 3):
            segment = pattern[i:i + 4]
            if None in segment:
                continue  # 跳过包含边界的段落
            if segment == [player, 0, player, player] or segment == [player, player, 0, player]:
                jump_three += 1

        # 检测跳四
        for i in range(len(pattern) - 4):
            segment = pattern[i:i + 5]
            if None in segment:
                continue  # 跳过包含边界的段落
            if segment == [player, player, player, 0, player] or segment == [player, player, 0, player, player]:
                jump_four += 1

        return jump_three, jump_four

    def check_connection(self, x: int, y: int, dx: int, dy: int, player: int) -> Tuple[int, int, int, Tuple[int, int]]:
        """
        检查棋子的连接情况。

        Args:
            x (int): 棋子x坐标。
            y (int): 棋子y坐标。
            dx (int): x方向增量。
            dy (int): y方向增量。
            player (int): 玩家标识。

        Returns:
            Tuple[int, int, int, Tuple[int, int]]: (连接数量, 活连接数量, 死连接数量, 端点状态)
        """
        count = 1  # 包括当前点
        live_count = 0
        dead_count = 0
        ends = [0, 0]

        # 正方向
        nx, ny = x + dx, y + dy
        while 0 <= nx < self.board_len and 0 <= ny < self.board_len and self.state[nx, ny] == player:
            count += 1
            nx += dx
            ny += dy
        if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
            ends[0] = self.state[nx, ny]
            if ends[0] == 0:
                live_count += 1
            else:
                dead_count += 1
        else:
            dead_count += 1

        # 反方向
        nx, ny = x - dx, y - dy
        while 0 <= nx < self.board_len and 0 <= ny < self.board_len and self.state[nx, ny] == player:
            count += 1
            nx -= dx
            ny -= dy
        if 0 <= nx < self.board_len and 0 <= ny < self.board_len:
            ends[1] = self.state[nx, ny]
            if ends[1] == 0:
                live_count += 1
            else:
                dead_count += 1
        else:
            dead_count += 1

        return count, live_count, dead_count, tuple(ends)

    def check_opponent_block(self, x: int, y: int, dx: int, dy: int, player: int) -> int:
        """
        检查对手的阻挡情况。

        Args:
            x (int): 棋子x坐标。
            y (int): 棋子y坐标。
            dx (int): x方向增量。
            dy (int): y方向增量。
            player (int): 玩家标识。

        Returns:
            int: 阻挡对手的棋子数量。
        """
        opponent = -player
        length = 1
        # 正方向
        nx, ny = x + dx, y + dy
        while 0 <= nx < self.board_len and 0 <= ny < self.board_len and self.state[nx, ny] == opponent:
            length += 1
            nx += dx
            ny += dy

        # 反方向
        nx, ny = x - dx, y - dy
        while 0 <= nx < self.board_len and 0 <= ny < self.board_len and self.state[nx, ny] == opponent:
            length += 1
            nx -= dx
            ny -= dy

        return length


# ----------------------------
# 二、定义节点类 Node
# ----------------------------
class Node:
    __slots__ = ['parent', 'children', 'c_puct', 'P', 'N', 'W', 'Q', 'U', 'score']

    def __init__(self, parent=None, prior_prob=0.0, c_puct=1.4):
        self.parent = parent
        self.children = {}
        self.c_puct = c_puct
        self.P = prior_prob
        self.N = 0
        self.W = 0.0
        self.Q = 0.0
        self.U = 0.0
        self.score = 0.0

    def select(self) -> Tuple[int, 'Node']:
        max_score = -float('inf')
        best_action = -1
        best_child = None
        for action, child in self.children.items():
            score = child.get_score()
            if score > max_score:
                max_score = score
                best_action = action
                best_child = child
        return best_action, best_child

    def expand(self, action_probs: List[Tuple[int, float]]):
        for action, prob in action_probs:
            if action not in self.children:
                self.children[action] = Node(parent=self, prior_prob=prob, c_puct=self.c_puct)

    def update(self, value: float):
        self.N += 1
        self.W += value
        self.Q = self.W / self.N

    def backup(self, value: float):
        self.update(value)
        if self.parent:
            self.parent.backup(-value)

    def is_leaf_node(self) -> bool:
        return len(self.children) == 0

    def get_score(self) -> float:
        if self.parent is not None and self.parent.N > 0:
            self.U = self.c_puct * self.P * math.sqrt(self.parent.N) / (1 + self.N)
        else:
            self.U = 0
        self.score = self.Q + self.U
        return self.score


# ----------------------------
# 三、定义策略-价值网络类 PolicyValueNet
# ----------------------------
class ResidualBlock(nn.Module):
    def __init__(self, channels=128):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.batch_norm2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        out = self.conv1(x)
        out = self.batch_norm1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.batch_norm2(out)
        out += x
        out = self.relu(out)
        return out


class PolicyHead(nn.Module):
    def __init__(self, in_channels=128, board_len=15):
        super(PolicyHead, self).__init__()
        self.conv = nn.Conv2d(in_channels, 4, kernel_size=1, bias=False)
        self.batch_norm = nn.BatchNorm2d(4)
        self.relu = nn.ReLU(inplace=True)
        self.fc = nn.Linear(4 * board_len * board_len, board_len * board_len)

    def forward(self, x):
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return F.log_softmax(x, dim=1)


class ValueHead(nn.Module):
    def __init__(self, in_channels=128, board_len=15):
        super(ValueHead, self).__init__()
        self.conv = nn.Conv2d(in_channels, 2, kernel_size=1, bias=False)
        self.batch_norm = nn.BatchNorm2d(2)
        self.relu = nn.ReLU(inplace=True)
        self.fc = nn.Sequential(
            nn.Linear(2 * board_len * board_len, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 1),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.conv(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class PolicyValueNet(nn.Module):
    def __init__(self, board_len=15, n_feature_planes=4):
        super(PolicyValueNet, self).__init__()
        self.board_len = board_len
        self.conv = nn.Sequential(
            nn.Conv2d(n_feature_planes, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        self.residues = nn.Sequential(
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
        )
        self.policy_head = PolicyHead(in_channels=128, board_len=board_len)
        self.value_head = ValueHead(in_channels=128, board_len=board_len)

    def forward(self, x):
        x = self.conv(x)
        x = self.residues(x)
        log_p, v = self.policy_head(x), self.value_head(x)
        return log_p, v

    def predict(self, chess_board: ChessBoard, device: torch.device):
        """
        使用策略-价值网络预测动作概率和价值。

        Args:
            chess_board (ChessBoard): 当前棋盘状态。
            device (torch.device): 设备信息。

        Returns:
            Tuple[np.ndarray, float]: (动作概率分布, 价值)
        """
        state = chess_board.get_feature_planes().unsqueeze(0).to(device)
        with torch.no_grad():
            log_p, v = self.forward(state)
        p = torch.exp(log_p).squeeze(0).cpu()
        available = torch.tensor(chess_board.get_available_actions(), dtype=torch.long)
        p_available = p[available]
        if p_available.sum() > 0:
            p_available = p_available / p_available.sum()
        else:
            p_available = torch.ones_like(p_available) / len(p_available)
        pi_full = torch.zeros(self.board_len * self.board_len, dtype=torch.float32)
        pi_full[available] = p_available
        return pi_full.numpy(), v.item()

    def save_model(self, path: str):
        """
        保存模型参数到指定路径。

        Args:
            path (str): 模型保存路径。
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save({
            'model_state_dict': self.state_dict(),
        }, path)
        logging.info(f"Model saved to {path}")

    def load_model(self, path: str, device: torch.device):
        """
        从指定路径加载模型参数。

        Args:
            path (str): 模型加载路径。
            device (torch.device): 设备信息。
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model path {path} does not exist.")
        checkpoint = torch.load(path, map_location=device)
        self.load_state_dict(checkpoint['model_state_dict'])
        self.to(device)
        self.eval()
        logging.info(f"Model loaded from {path}")


# ----------------------------
# 四、定义AlphaZeroMCTS类
# ----------------------------
class AlphaZeroMCTS:
    def __init__(self, policy_value_net: PolicyValueNet, c_puct: float = 2.5, n_iters: int = 100,
                 is_self_play: bool = True, device: torch.device = torch.device("cpu")):
        """
        初始化AlphaZero的MCTS。

        Args:
            policy_value_net (PolicyValueNet): 策略-价值网络。
            c_puct (float): PUCT算法中的探索常数。
            n_iters (int): 每步的MCTS迭代次数。
            is_self_play (bool): 是否用于自我博弈。
            device (torch.device): 设备信息。
        """
        self.policy_value_net = policy_value_net
        self.c_puct = c_puct
        self.n_iters = n_iters
        self.is_self_play = is_self_play
        self.device = device
        self.reset_root()

    def reset_root(self):
        """
        重置MCTS的根节点。
        """
        self.root = Node()

    def search(self, chess_board: ChessBoard):
        """
        执行一次MCTS搜索。

        Args:
            chess_board (ChessBoard): 当前棋盘状态。
        """
        node = self.root
        board = chess_board.clone()
        while not node.is_leaf_node():
            try:
                action, node = node.select()
                board.do_action(action)
            except ValueError as e:
                logging.error(f"MCTS search error: {e}")
                return
        game_over, winner = board.is_game_over()
        if not game_over:
            try:
                action_probs, value = self.policy_value_net.predict(board, self.device)
                available_actions = board.get_available_actions()
                action_prob_pairs = list(zip(available_actions, action_probs[available_actions]))
                node.expand(action_prob_pairs)
            except Exception as e:
                logging.error(f"Error during node expansion: {e}")
                value = 0
        else:
            value = 0 if winner is None else (1 if winner == -board.current_player else -1)
        node.backup(value)

    def get_action(self, chess_board: ChessBoard, temperature: float = 1.0) -> Tuple[int, np.ndarray]:
        """
        根据当前棋盘状态选择一个动作。

        Args:
            chess_board (ChessBoard): 当前棋盘状态。
            temperature (float): 温度参数，用于控制策略分布的随机性。

        Returns:
            Tuple[int, np.ndarray]: (选择的动作, 策略分布)
        """
        self.reset_root()
        for _ in range(self.n_iters):
            self.search(chess_board)

        actions = list(self.root.children.keys())
        visit_counts = np.array([self.root.children[a].N for a in actions])
        if visit_counts.sum() == 0:
            pi = np.ones_like(visit_counts) / len(visit_counts)
            best_action = np.random.choice(actions)
        else:
            if temperature == 0:
                best_action_idx = np.argmax(visit_counts)
                best_action = actions[best_action_idx]
                pi = np.zeros(len(actions))
                pi[best_action_idx] = 1
            else:
                pi = visit_counts ** (1 / temperature)
                pi /= pi.sum()
                best_action = np.random.choice(actions, p=pi)

        pi_full = np.zeros(chess_board.board_len * chess_board.board_len, dtype=np.float32)
        for idx, a in enumerate(actions):
            pi_full[a] = pi[idx]

        return best_action, pi_full


# ----------------------------
# 五、自我博弈数据类
# ----------------------------
class SelfPlayData(NamedTuple):
    pi_list: List[np.ndarray]
    z_list: List[float]
    feature_planes_list: List[np.ndarray]


def self_play_game(chess_board: ChessBoard, mcts: AlphaZeroMCTS, move_count_threshold: int = 30,
                  max_moves: int = 100) -> SelfPlayData:
    """
    进行一场自我博弈游戏。

    Args:
        chess_board (ChessBoard): 棋盘实例。
        mcts (AlphaZeroMCTS): MCTS实例。
        move_count_threshold (int): 在达到该步数之前使用高温度。
        max_moves (int): 游戏的最大步数。

    Returns:
        SelfPlayData: 自我博弈生成的数据。
    """
    pi_list = []
    z_list = []
    state_feature_planes_list = []
    move_count = 0
    while True:
        state_feature_planes = chess_board.get_feature_planes()
        state_feature_planes_list.append(state_feature_planes.numpy())
        temperature = 1.0 if move_count < move_count_threshold else 0.0
        action, pi = mcts.get_action(chess_board, temperature)
        pi_list.append(pi)
        try:
            chess_board.do_action(action)  # 自动切换玩家
        except ValueError as e:
            logging.error(f"Error during do_action: {e}")
            break
        move_count += 1
        game_over, winner = chess_board.is_game_over()
        if game_over or move_count >= max_moves:
            if winner is None:
                z = 0
            else:
                z = 1 if winner == 1 else -1
            # 反转结果以适应每一步的玩家
            z_list = [z * ((-1) ** i) for i in range(len(pi_list))]
            return SelfPlayData(pi_list, z_list, state_feature_planes_list)


# ----------------------------
# 六、数据增强与训练
# ----------------------------
def rotate(data: np.ndarray, board_len: int) -> np.ndarray:
    """
    旋转特征平面和策略分布90度。

    Args:
        data (np.ndarray): 数据，形状为 (n_feature_planes, board_len, board_len)。
        board_len (int): 棋盘大小。

    Returns:
        np.ndarray: 旋转后的数据。
    """
    return np.rot90(data, k=1, axes=(1, 2))


def flip(data: np.ndarray, board_len: int) -> np.ndarray:
    """
    水平翻转特征平面和策略分布。

    Args:
        data (np.ndarray): 数据，形状为 (n_feature_planes, board_len, board_len)。
        board_len (int): 棋盘大小。

    Returns:
        np.ndarray: 翻转后的数据。
    """
    return np.flip(data, axis=2)


def augment_data(pi: np.ndarray, feature_planes: np.ndarray, board_len: int) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    对数据进行旋转和翻转增强。

    Args:
        pi (np.ndarray): 策略分布，形状为 (board_len * board_len, )。
        feature_planes (np.ndarray): 特征平面，形状为 (n_feature_planes, board_len, board_len)。
        board_len (int): 棋盘大小。

    Returns:
        List[Tuple[np.ndarray, np.ndarray]]: 增强后的(pi, feature_planes)对列表。
    """
    augmented = []
    for _ in range(4):
        # 旋转
        feature_rotated = rotate(feature_planes, board_len)
        pi_rotated = np.rot90(pi.reshape(board_len, board_len), k=1).flatten()
        augmented.append((pi_rotated.copy(), feature_rotated.copy()))

        # 翻转
        feature_flipped = flip(feature_rotated, board_len)
        pi_flipped = np.flip(pi_rotated.reshape(board_len, board_len), axis=1).flatten()
        augmented.append((pi_flipped.copy(), feature_flipped.copy()))
    return augmented


def visualize_augmentation(pi: np.ndarray, feature_planes: np.ndarray, board_len: int,
                           title: str = "Data Augmentation"):
    """
    可视化数据增强结果。

    Args:
        pi (np.ndarray): 策略分布，形状为 (board_len * board_len, )。
        feature_planes (np.ndarray): 特征平面，形状为 (n_feature_planes, board_len, board_len)。
        board_len (int): 棋盘大小。
        title (str): 可视化标题。
    """
    num_feature_planes = feature_planes.shape[0]
    fig, axes = plt.subplots(1, num_feature_planes + 1, figsize=(15, 3))
    fig.suptitle(title)

    for i in range(num_feature_planes):
        ax = axes[i]
        im = ax.imshow(feature_planes[i], cmap='gray')
        ax.set_title(f'Feature Plane {i}')
        ax.axis('off')
        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    # 可视化策略分布
    ax = axes[-1]
    pi_grid = pi.reshape(board_len, board_len)
    im = ax.imshow(pi_grid, cmap='hot', interpolation='nearest')
    ax.set_title('Policy Distribution')
    ax.axis('off')
    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)

    plt.show()


def verify_data_augmentation(data_buffer, board_len=15, num_samples: int = 5):
    """
    验证数据增强的正确性，通过可视化部分增强后的数据。

    Args:
        data_buffer (List[SelfPlayData]): 自我博弈生成的数据缓冲区。
        board_len (int): 棋盘大小。
        num_samples (int): 要验证的样本数量。
    """
    samples_verified = 0
    for game_data in data_buffer:
        for pi, fp, z in zip(game_data.pi_list, game_data.feature_planes_list, game_data.z_list):
            augmented = augment_data(pi, fp, board_len)
            for idx, (pi_aug, fp_aug) in enumerate(augmented):
                if samples_verified >= num_samples:
                    return
                title = f"Sample {samples_verified + 1}, Augmentation {idx + 1}"
                visualize_augmentation(pi_aug, fp_aug, board_len, title=title)
                samples_verified += 1
                if samples_verified >= num_samples:
                    return


class SelfPlayDataset(Dataset):
    def __init__(self, data_buffer, board_len):
        """
        初始化自我博弈数据集。

        Args:
            data_buffer (List[SelfPlayData]): 自我博弈生成的数据缓冲区。
            board_len (int): 棋盘大小。
        """
        self.data = []
        self.board_len = board_len
        for game_data in data_buffer:
            for pi, fp, z in zip(game_data.pi_list, game_data.feature_planes_list, game_data.z_list):
                augmented = augment_data(pi, fp, board_len)
                for pi_aug, fp_aug in augmented:
                    self.data.append((pi_aug, fp_aug, z))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pi, fp, z = self.data[idx]
        return torch.tensor(fp, dtype=torch.float32), torch.tensor(pi, dtype=torch.float32), torch.tensor(z,
                                                                                                          dtype=torch.float32)


def initialize_weights(module):
    """
    初始化神经网络权重。

    Args:
        module (nn.Module): 神经网络模块。
    """
    if isinstance(module, nn.Conv2d):
        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(module, nn.BatchNorm2d):
        nn.init.constant_(module.weight, 1)
        nn.init.constant_(module.bias, 0)
    elif isinstance(module, nn.Linear):
        nn.init.xavier_uniform_(module.weight)
        nn.init.constant_(module.bias, 0)


def train(policy_value_net: PolicyValueNet, data_buffer: List[SelfPlayData], board_len=15,
          epochs: int = 20, batch_size: int = 256, lr: float = 5e-4, patience: int = 5):
    """
    训练策略-价值网络。

    Args:
        policy_value_net (PolicyValueNet): 策略-价值网络实例。
        data_buffer (List[SelfPlayData]): 自我博弈生成的数据缓冲区。
        board_len (int): 棋盘大小。
        epochs (int): 训练轮数。
        batch_size (int): 批次大小。
        lr (float): 学习率。
        patience (int): 早停的耐心参数。
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    policy_value_net.to(device)
    optimizer = torch.optim.Adam(policy_value_net.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    mse_loss = nn.MSELoss()

    # 使用混合精度训练
    scaler = GradScaler()

    policy_value_net.train()

    dataset = SelfPlayDataset(data_buffer, board_len)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,  # 根据CPU核心数调整
        pin_memory=True if device.type == 'cuda' else False
    )

    best_loss = float('inf')
    patience_counter = 0
    checkpoint_path = "checkpoints/"
    os.makedirs(checkpoint_path, exist_ok=True)

    for epoch in range(epochs):
        start_time = time.time()
        total_loss = 0.0
        # 使用 tqdm 显示训练进度
        for fp, pi, z in tqdm(dataloader, desc=f"Training Epoch {epoch + 1}/{epochs}", unit="batch"):
            fp, pi, z = fp.to(device), pi.to(device), z.to(device).unsqueeze(1)

            optimizer.zero_grad()
            with autocast():
                log_p, v = policy_value_net(fp)
                loss_policy = -torch.sum(pi * log_p, dim=1).mean()
                loss_value = mse_loss(v, z)
                loss = loss_value + 0.01 * loss_policy

            scaler.scale(loss).backward()
            # 梯度裁剪
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(policy_value_net.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        scheduler.step(avg_loss)
        elapsed_time = time.time() - start_time
        logging.info(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Time: {elapsed_time:.2f}s")

        # 早停检查
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            # 保存最佳模型
            policy_value_net.save_model(os.path.join(checkpoint_path, "best_policy_value_net.pth"))
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logging.info("Early stopping triggered.")
                break

        # 保存每个epoch的检查点
        policy_value_net.save_model(os.path.join(checkpoint_path, f"policy_value_net_epoch_{epoch + 1}.pth"))

    logging.info("Training finished.")


# ----------------------------
# 七、手动测试函数
# ----------------------------
def manual_test():
    """
    进行手动测试，放置棋子并检查高级模式和连接情况。
    """
    board_len = 15
    n_feature_planes = 4
    chess_board = ChessBoard(board_len=board_len, n_feature_planes=n_feature_planes)
    player = 1

    # 手动放置五个棋子水平放置在(7,7)到(7,11)
    actions = [7 * board_len + y for y in range(7, 12)]
    for action in actions:
        chess_board.do_action(action, player=player)

    # 可视化棋盘
    visualize_board(chess_board)

    # 检查游戏是否结束
    game_over, winner = chess_board.is_game_over()
    if game_over:
        if winner is None:
            print("游戏结束，平局。")
        else:
            print(f"游戏结束，玩家 {winner} 获胜。")
    else:
        print("游戏尚未结束。")

    # 检查高级模式
    jump_three, jump_four = chess_board.check_advanced_patterns(7, 11, 0, 1, player)
    print(f"Jump Three: {jump_three}, Jump Four: {jump_four}")

    # 检查连接情况
    conn_info = chess_board.check_connection(7, 9, 0, 1, player)
    print(f"Connection Info at (7,9): count={conn_info[0]}, live={conn_info[1]}, dead={conn_info[2]}, ends={conn_info[3]}")


def visualize_board(board: ChessBoard, title: str = "Chess Board"):
    """
    可视化当前棋盘状态。

    Args:
        board (ChessBoard): 当前棋盘。
        title (str): 可视化标题。
    """
    board_grid = np.zeros((board.board_len, board.board_len))
    for x in range(board.board_len):
        for y in range(board.board_len):
            if board.state[x, y] == 1:
                board_grid[x, y] = 1
            elif board.state[x, y] == -1:
                board_grid[x, y] = -1
    plt.imshow(board_grid, cmap='bwr', origin='upper')
    plt.title(title)
    plt.xticks(range(board.board_len))
    plt.yticks(range(board.board_len))
    plt.grid(True)
    plt.show()


# ----------------------------
# 八、主函数
# ----------------------------
def main():
    board_len = 15
    n_feature_planes = 4  # 假设增加了启发式评分平面
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    policy_value_net = PolicyValueNet(board_len=board_len, n_feature_planes=n_feature_planes).to(device)
    initialize_weights(policy_value_net)

    # 可选：加载已有模型
    # try:
    #     policy_value_net.load_model("policy_value_net.pth", device)
    # except Exception as e:
    #     logging.error(f"Error loading model: {e}")

    mcts = AlphaZeroMCTS(policy_value_net=policy_value_net, c_puct=2.5, n_iters=50, is_self_play=True, device=device)

    data_buffer = []
    n_self_play_games = 500
    max_moves_per_game = 50
    move_count_threshold = 10

    start_time = time.time()
    # 使用 tqdm 显示自我博弈进度
    for game in tqdm(range(n_self_play_games), desc="Self-Play Games", unit="game"):
        chess_board = ChessBoard(board_len=board_len, n_feature_planes=n_feature_planes)
        try:
            self_play_data = self_play_game(chess_board, mcts, move_count_threshold, max_moves_per_game)
            data_buffer.append(self_play_data)
        except Exception as e:
            logging.error(f"Error during self-play game {game + 1}: {e}")
            continue
    total_self_play_time = time.time() - start_time
    logging.info(f"Completed {n_self_play_games} self-play games in {total_self_play_time:.2f} seconds.")

    # 验证数据增强的正确性
    logging.info("Verifying data augmentation...")
    try:
        verify_data_augmentation(data_buffer, board_len=board_len, num_samples=5)
    except Exception as e:
        logging.error(f"Error during data augmentation verification: {e}")
    logging.info("Data augmentation verification completed.")

    # 开始训练
    logging.info("Starting training...")
    try:
        train(policy_value_net, data_buffer, board_len=board_len, epochs=20, batch_size=256, lr=5e-4, patience=5)
    except Exception as e:
        logging.error(f"Error during training: {e}")
    logging.info("Training completed.")

    # 保存模型
    try:
        policy_value_net.save_model("policy_value_net.pth")
    except Exception as e:
        logging.error(f"Error saving model: {e}")
    logging.info("Model saved as 'policy_value_net.pth'.")

    # 可选：加载模型
    # try:
    #     policy_value_net.load_model("policy_value_net.pth", device)
    # except Exception as e:
    #     logging.error(f"Error loading model: {e}")

    # 手动测试
    logging.info("Starting manual test...")
    manual_test()
    logging.info("Manual test completed.")
    
    
    


# ----------------------------
# 九、执行
# ---------------------------- b]0----------
if __name__ == "__main__":
    main()
